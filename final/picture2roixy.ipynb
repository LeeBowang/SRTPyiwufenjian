{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import  libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# from __future__ import absolute_import\n",
    "# import random\n",
    "# import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "# from optparse import OptionParser\n",
    "import pickle\n",
    "# import math\n",
    "import cv2\n",
    "# import copy\n",
    "# from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "# import pandas as pd\n",
    "import os\n",
    "\n",
    "import socket\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "\n",
    "import serial\n",
    "\n",
    "import serial.tools.list_ports\n",
    "\n",
    "# from sklearn.metrics import average_precision_score\n",
    "from keras import backend as K\n",
    "# from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "# from keras.engine.topology import get_source_inputs\n",
    "# from keras.utils import layer_utils\n",
    "# from keras.utils.data_utils import get_file\n",
    "# from keras.objectives import categorical_crossentropy\n",
    "\n",
    "from keras.models import Model\n",
    "# from keras.utils import generic_utils\n",
    "from keras.engine import Layer, InputSpec\n",
    "# from keras import initializers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/hou/wzx_frcnn/Faster_RCNN_for_Open_Images_Dataset_Keras'\n",
    "# Test data (annotation file)\n",
    "#test_path = '/home/device/bo/wzx_frcnn/Dataset/Open Images Dataset v5 (Bounding Boxes)/person_car_phone_test_annotation.txt' \n",
    "# Directory to save the test images\n",
    "test_base_path = '/home/hou/wzx_frcnn/Dataset/Open Images Dataset v5 (Bounding Boxes)/test' \n",
    "\n",
    "#config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')\n",
    "config_output_filename = '/home/hou/wzx_frcnn/Faster_RCNN_for_Open_Images_Dataset_Keras/model_vgg_config.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "portList = list(serial.tools.list_ports.comports())\n",
    "\n",
    "INIT_CMD=[0xFF,0xFE,0xFD,0xFC,0x01,0x08,0x02,0x01,0x00,0x00,0x00,0x00,0x00,0xFB] #初始化\n",
    "POSCRT_CMD_1=[0xFF,0xFE,0xFD,0xFC,0x01,0x06,0x02,0x01,0x00,0x3C,0x00,0x00,0x00,0xFB] #设置60%夹持位置\n",
    "READ_CURRT_STATE=[0xFF,0xFE,0xFD,0xFC,0x01,0x0F,0x01,0x00,0x00,0x00,0x00,0x00,0x00,0xFB] #读取当前状态，读操作\n",
    "\n",
    "\n",
    "'''\n",
    "AG95 grriper action\n",
    "'''\n",
    "\n",
    "\n",
    "class Ag95Commend(object):\n",
    "    \n",
    "\n",
    "    # initializ Ag95 parameter\n",
    "    # comName : which com port used connect to Ag95 default = com4 （在com4连接）\n",
    "    # baud rate : dafault = 115200\n",
    "    def __init__(self):\n",
    "        print('instantiate Ag95')\n",
    "    \n",
    "    def openSerial(self,comName='COM17',baudrate=115200):\n",
    "        self.ser=serial.Serial(comName,baudrate,timeout=60)\n",
    "        \n",
    "        if self.ser.name == '/dev/ttyACM0':\n",
    "            print('serial connected to ag95')\n",
    "        else:\n",
    "            raise Exception('fail to connect to ag95, check your com port')\n",
    "    # initialize ag95 \n",
    "    def getStatus(self):\n",
    "        tmp=self.ser.write(READ_CURRT_STATE)\n",
    "        time.sleep(0.2)\n",
    "        r=self.ser.read(14)\n",
    "        return bytearray.fromhex(r.hex())                #（读取目前状态） 前面都是连接检测\n",
    "    \n",
    "    def init(self):\n",
    "        print('initializing Ag95 grriper')\n",
    "        self.ser.write(INIT_CMD)\n",
    "        ret=self.ser.read(14)\n",
    "        self.waitActionDone()\n",
    "        if ret.hex()=='fffefdfc010802010000000000fb':   #（读取是否主动反馈，读操作）\n",
    "            print('init AG2 gripper successful')\n",
    "        else:\n",
    "            print('failure to init AG2')\n",
    "            \n",
    "    def waitActionDone(self):\n",
    "        \n",
    "        r=self.getStatus()\n",
    "        print(r)\n",
    "        print('waiting action')\n",
    "        while(r.hex()=='fffefdfc010f01000000000000fb'):    #（读取当前状态）\n",
    "            r=self.getStatus()\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            #print(r.hex())\n",
    "        print('action done')\n",
    "        return True\n",
    "    \n",
    "    def setPosition(self,pos=60):                           #（设置夹持位置 与x有关）\n",
    "        hex_pos_str=hex(pos).split('x')[1]\n",
    "        if len(hex_pos_str)<2:\n",
    "            hex_pos_str='0'+hex_pos_str\n",
    "        #print(hex_pos_str)\n",
    "        cmd='FFFEFDFC0106020100'+hex_pos_str+'000000FB'\n",
    "        #cmd=[0xFF,0xFE,0xFD,0xFC,0x01,0x06,0x02,0x01,0x00,int(),0x00,0x00,0x00,0xFB]\n",
    "        \n",
    "        cmd_byte=bytearray.fromhex(cmd)\n",
    "        self.ser.write(cmd_byte)\n",
    "        \n",
    "\n",
    "    \n",
    "    def closeSerial(self):\n",
    "        self.ser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiate Ag95\n",
      "serial connected to ag95\n",
      "initializing Ag95 grriper\n",
      "bytearray(b'\\xff\\xfe\\xfd\\xfc\\x01\\x0f\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xfb')\n",
      "waiting action\n",
      "action done\n",
      "init AG2 gripper successful\n"
     ]
    }
   ],
   "source": [
    "comName='/dev/ttyACM0'\n",
    "grriper=Ag95Commend()\n",
    "grriper.openSerial(comName=comName)\n",
    "grriper.init()\n",
    "# success=bytearray.fromhex('FFFEFDFC010F01000003000000FB')\n",
    "# failure=bytearray.fromhex('FFFEFDFC010F01000003000000FB')\n",
    "grriper.setPosition(10)\n",
    "time.sleep(1)\n",
    "grriper.setPosition(90)\n",
    "time.sleep(1)\n",
    "grriper.setPosition(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\n",
    "\t\t# Print the process or not\n",
    "\t\tself.verbose = True\n",
    "\n",
    "\t\t# Name of base network\n",
    "\t\tself.network = 'vgg'\n",
    "\n",
    "\t\t# Setting for data augmentation\n",
    "\t\tself.use_horizontal_flips = False\n",
    "\t\tself.use_vertical_flips = False\n",
    "\t\tself.rot_90 = False\n",
    "\n",
    "\t\t# Anchor box scales\n",
    "    # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "    # Original anchor_box_scales in the paper is [128, 256, 512]\n",
    "\t\tself.anchor_box_scales = [64, 128, 256] \n",
    "\n",
    "\t\t# Anchor box ratios\n",
    "\t\tself.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "\t\t# Size to resize the smallest side of the image\n",
    "\t\t# Original setting in paper is 600. Set to 300 in here to save training time\n",
    "\t\tself.im_size = 300\n",
    "\n",
    "\t\t# image channel-wise mean to subtract\n",
    "\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "\t\tself.img_scaling_factor = 1.0\n",
    "\n",
    "\t\t# number of ROIs at once\n",
    "\t\tself.num_rois = 4\n",
    "\n",
    "\t\t# stride at the RPN (this depends on the network configuration)\n",
    "\t\tself.rpn_stride = 16\n",
    "\n",
    "\t\tself.balanced_classes = False\n",
    "\n",
    "\t\t# scaling the stdev\n",
    "\t\tself.std_scaling = 4.0\n",
    "\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "\t\t# overlaps for RPN\n",
    "\t\tself.rpn_min_overlap = 0.3\n",
    "\t\tself.rpn_max_overlap = 0.7\n",
    "\n",
    "\t\t# overlaps for classifier ROIs\n",
    "\t\tself.classifier_min_overlap = 0.1\n",
    "\t\tself.classifier_max_overlap = 0.5\n",
    "\n",
    "\t\t# placeholder for the class mapping, automatically generated by the parser\n",
    "\t\tself.class_mapping = None\n",
    "\n",
    "\t\tself.model_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define RoiPoolingConv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoiPoolingConv(Layer):\n",
    "    '''ROI pooling layer for 2D inputs.\n",
    "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
    "    K. He, X. Zhang, S. Ren, J. Sun\n",
    "    # Arguments\n",
    "        pool_size: int\n",
    "            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n",
    "        num_rois: number of regions of interest to be used\n",
    "    # Input shape\n",
    "        list of two 4D tensors [X_img,X_roi] with shape:\n",
    "        X_img:\n",
    "        `(1, rows, cols, channels)`\n",
    "        X_roi:\n",
    "        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
    "    # Output shape\n",
    "        3D tensor with shape:\n",
    "        `(1, num_rois, channels, pool_size, pool_size)`\n",
    "    '''\n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "\n",
    "        self.dim_ordering = K.image_dim_ordering()\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[0][3]   \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        assert(len(x) == 2)\n",
    "\n",
    "        # x[0] is image with shape (rows, cols, channels)\n",
    "        img = x[0]\n",
    "\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "\n",
    "        input_shape = K.shape(img)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for roi_idx in range(self.num_rois):\n",
    "\n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "\n",
    "            x = K.cast(x, 'int32')\n",
    "            y = K.cast(y, 'int32')\n",
    "            w = K.cast(w, 'int32')\n",
    "            h = K.cast(h, 'int32')\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
    "            outputs.append(rs)\n",
    "                \n",
    "\n",
    "        final_output = K.concatenate(outputs, axis=0)\n",
    "\n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "\n",
    "        # permute_dimensions is similar to transpose\n",
    "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nn_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_base(input_tensor=None, trainable=False):\n",
    "\n",
    "\n",
    "    input_shape = (None, None, 3)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    bn_axis = 3\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RPN_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_layer(base_layers, num_anchors):\n",
    "    \"\"\"Create a rpn layer\n",
    "        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer\n",
    "                Keep the padding 'same' to preserve the feature map's size\n",
    "        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer\n",
    "                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output\n",
    "                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation\n",
    "    Args:\n",
    "        base_layers: vgg in here\n",
    "        num_anchors: 9 in here\n",
    "\n",
    "    Returns:\n",
    "        [x_class, x_regr, base_layers]\n",
    "        x_class: classification for whether it's an object\n",
    "        x_regr: bboxes regression\n",
    "        base_layers: vgg in here\n",
    "    \"\"\"\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "\n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "\n",
    "    return [x_class, x_regr, base_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classifier_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n",
    "    \"\"\"Create a classifier layer\n",
    "    \n",
    "    Args:\n",
    "        base_layers: vgg\n",
    "        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
    "        num_rois: number of rois to be processed in one time (4 in here)\n",
    "\n",
    "    Returns:\n",
    "        list(out_class, out_regr)\n",
    "        out_class: classifier layer output\n",
    "        out_regr: regression layer output\n",
    "    \"\"\"\n",
    "\n",
    "    input_shape = (num_rois,7,7,512)\n",
    "\n",
    "    pooling_regions = 7\n",
    "\n",
    "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
    "    # num_rois (4) 7x7 roi pooling\n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
    "\n",
    "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
    "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "\n",
    "    # There are two output layer\n",
    "    # out_class: softmax acivation function for classify the class name of the object\n",
    "    # out_regr: linear activation function for bboxes coordinates regression\n",
    "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
    "    # note: no regression target for bg class\n",
    "    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
    "\n",
    "    return [out_class, out_regr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n",
    "\t\"\"\"Convert rpn layer to roi bboxes\n",
    "\n",
    "\tArgs: (num_anchors = 9)\n",
    "\t\trpn_layer: output layer for rpn classification \n",
    "\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n",
    "\t\t\tMight be (1, 18, 25, 9) if resized image is 400 width and 300\n",
    "\t\tregr_layer: output layer for rpn regression\n",
    "\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n",
    "\t\t\tMight be (1, 18, 25, 36) if resized image is 400 width and 300\n",
    "\t\tC: config\n",
    "\t\tuse_regr: Wether to use bboxes regression in rpn\n",
    "\t\tmax_boxes: max bboxes number for non-max-suppression (NMS)\n",
    "\t\toverlap_thresh: If iou in NMS is larger than this threshold, drop the box\n",
    "\n",
    "\tReturns:\n",
    "\t\tresult: boxes from non-max-suppression (shape=(300, 4))\n",
    "\t\t\tboxes: coordinates for bboxes (on the feature map)\n",
    "\t\"\"\"\n",
    "\tregr_layer = regr_layer / C.std_scaling\n",
    "\n",
    "\tanchor_sizes = C.anchor_box_scales   # (3 in here)\n",
    "\tanchor_ratios = C.anchor_box_ratios  # (3 in here)\n",
    "\n",
    "\tassert rpn_layer.shape[0] == 1\n",
    "\n",
    "\t(rows, cols) = rpn_layer.shape[1:3]\n",
    "\n",
    "\tcurr_layer = 0\n",
    "\n",
    "\t# A.shape = (4, feature_map.height, feature_map.width, num_anchors) \n",
    "\t# Might be (4, 18, 25, 9) if resized image is 400 width and 300\n",
    "\t# A is the coordinates for 9 anchors for every point in the feature map \n",
    "\t# => all 18x25x9=4050 anchors cooridnates\n",
    "\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n",
    "\n",
    "\tfor anchor_size in anchor_sizes:\n",
    "\t\tfor anchor_ratio in anchor_ratios:\n",
    "\t\t\t# anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n",
    "\t\t\t# anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n",
    "\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n",
    "\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n",
    "\t\t\t\n",
    "\t\t\t# curr_layer: 0~8 (9 anchors)\n",
    "\t\t\t# the Kth anchor of all position in the feature map (9th in total)\n",
    "\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n",
    "\t\t\tregr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n",
    "\n",
    "\t\t\t# Create 18x25 mesh grid\n",
    "\t\t\t# For every point in x, there are all the y points and vice versa\n",
    "\t\t\t# X.shape = (18, 25)\n",
    "\t\t\t# Y.shape = (18, 25)\n",
    "\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n",
    "\n",
    "\t\t\t# Calculate anchor position and size for each feature map point\n",
    "\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n",
    "\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n",
    "\t\t\tA[2, :, :, curr_layer] = anchor_x       # width of current anchor\n",
    "\t\t\tA[3, :, :, curr_layer] = anchor_y       # height of current anchor\n",
    "\n",
    "\t\t\t# Apply regression to x, y, w and h if there is rpn regression layer\n",
    "\t\t\tif use_regr:\n",
    "\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n",
    "\n",
    "\t\t\t# Avoid width and height exceeding 1\n",
    "\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n",
    "\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n",
    "\n",
    "\t\t\t# Convert (x, y , w, h) to (x1, y1, x2, y2)\n",
    "\t\t\t# x1, y1 is top left coordinate\n",
    "\t\t\t# x2, y2 is bottom right coordinate\n",
    "\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n",
    "\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n",
    "\n",
    "\t\t\t# Avoid bboxes drawn outside the feature map\n",
    "\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n",
    "\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n",
    "\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n",
    "\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n",
    "\n",
    "\t\t\tcurr_layer += 1\n",
    "\n",
    "\tall_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n",
    "\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n",
    "\n",
    "\tx1 = all_boxes[:, 0]\n",
    "\ty1 = all_boxes[:, 1]\n",
    "\tx2 = all_boxes[:, 2]\n",
    "\ty2 = all_boxes[:, 3]\n",
    "\n",
    "\t# Find out the bboxes which is illegal and delete them from bboxes list\n",
    "\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
    "\n",
    "\tall_boxes = np.delete(all_boxes, idxs, 0)\n",
    "\tall_probs = np.delete(all_probs, idxs, 0)\n",
    "\n",
    "\t# Apply non_max_suppression\n",
    "\t# Only extract the bboxes. Don't need rpn probs in the later process\n",
    "\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n",
    "\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n",
    "\t# code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "\t# if there are no boxes, return an empty list\n",
    "  \n",
    "    # Process explanation:\n",
    "    #   Step 1: Sort the probs list\n",
    "    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list\n",
    "    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list\n",
    "    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list \n",
    "\tif len(boxes) == 0:\n",
    "\t\treturn []\n",
    "\n",
    "\t# grab the coordinates of the bounding boxes\n",
    "\tx1 = boxes[:, 0]\n",
    "\ty1 = boxes[:, 1]\n",
    "\tx2 = boxes[:, 2]\n",
    "\ty2 = boxes[:, 3]\n",
    "\n",
    "\tnp.testing.assert_array_less(x1, x2)\n",
    "\tnp.testing.assert_array_less(y1, y2)\n",
    "\n",
    "\t# if the bounding boxes integers, convert them to floats --\n",
    "\t# this is important since we'll be doing a bunch of divisions\n",
    "\tif boxes.dtype.kind == \"i\":\n",
    "\t\tboxes = boxes.astype(\"float\")\n",
    "\n",
    "\t# initialize the list of picked indexes\t\n",
    "\tpick = []\n",
    "\n",
    "\t# calculate the areas\n",
    "\tarea = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "\t# sort the bounding boxes \n",
    "\tidxs = np.argsort(probs)\n",
    "\n",
    "\t# keep looping while some indexes still remain in the indexes\n",
    "\t# list\n",
    "\twhile len(idxs) > 0:\n",
    "\t\t# grab the last index in the indexes list and add the\n",
    "\t\t# index value to the list of picked indexes\n",
    "\t\tlast = len(idxs) - 1\n",
    "\t\ti = idxs[last]\n",
    "\t\tpick.append(i)\n",
    "\n",
    "\t\t# find the intersection\n",
    "\n",
    "\t\txx1_int = np.maximum(x1[i], x1[idxs[:last]])\n",
    "\t\tyy1_int = np.maximum(y1[i], y1[idxs[:last]])\n",
    "\t\txx2_int = np.minimum(x2[i], x2[idxs[:last]])\n",
    "\t\tyy2_int = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "\t\tww_int = np.maximum(0, xx2_int - xx1_int)\n",
    "\t\thh_int = np.maximum(0, yy2_int - yy1_int)\n",
    "\n",
    "\t\tarea_int = ww_int * hh_int\n",
    "\n",
    "\t\t# find the union\n",
    "\t\tarea_union = area[i] + area[idxs[:last]] - area_int\n",
    "\n",
    "\t\t# compute the ratio of overlap\n",
    "\t\toverlap = area_int/(area_union + 1e-6)\n",
    "\n",
    "\t\t# delete all indexes from the index list that have\n",
    "\t\tidxs = np.delete(idxs, np.concatenate(([last],\n",
    "\t\t\tnp.where(overlap > overlap_thresh)[0])))\n",
    "\n",
    "\t\tif len(pick) >= max_boxes:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# return only the bounding boxes that were picked using the integer data type\n",
    "\tboxes = boxes[pick].astype(\"int\")\n",
    "\tprobs = probs[pick]\n",
    "\treturn boxes, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_regr_np(X, T):\n",
    "\t\"\"\"Apply regression layer to all anchors in one feature map\n",
    "\n",
    "\tArgs:\n",
    "\t\tX: shape=(4, 18, 25) the current anchor type for all points in the feature map\n",
    "\t\tT: regression layer shape=(4, 18, 25)\n",
    "\n",
    "\tReturns:\n",
    "\t\tX: regressed position and size for current anchor\n",
    "\t\"\"\"\n",
    "\ttry:\n",
    "\t\tx = X[0, :, :]\n",
    "\t\ty = X[1, :, :]\n",
    "\t\tw = X[2, :, :]\n",
    "\t\th = X[3, :, :]\n",
    "\n",
    "\t\ttx = T[0, :, :]\n",
    "\t\tty = T[1, :, :]\n",
    "\t\ttw = T[2, :, :]\n",
    "\t\tth = T[3, :, :]\n",
    "\n",
    "\t\tcx = x + w/2.\n",
    "\t\tcy = y + h/2.\n",
    "\t\tcx1 = tx * w + cx\n",
    "\t\tcy1 = ty * h + cy\n",
    "\n",
    "\t\tw1 = np.exp(tw.astype(np.float64)) * w\n",
    "\t\th1 = np.exp(th.astype(np.float64)) * h\n",
    "\t\tx1 = cx1 - w1/2.\n",
    "\t\ty1 = cy1 - h1/2.\n",
    "\n",
    "\t\tx1 = np.round(x1)\n",
    "\t\ty1 = np.round(y1)\n",
    "\t\tw1 = np.round(w1)\n",
    "\t\th1 = np.round(h1)\n",
    "\t\treturn np.stack([x1, y1, w1, h1])\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(config_output_filename, 'rb') as f_in:\n",
    "\tC = pickle.load(f_in)\n",
    "C.model_path = '/home/hou/wzx_frcnn/Faster_RCNN_for_Open_Images_Dataset_Keras/model/model_frcnn_vgg.hdf5'\n",
    "\n",
    "# turn off any data augmentation at test time\n",
    "C.use_horizontal_flips = False\n",
    "C.use_vertical_flips = False\n",
    "C.rot_90 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_img_size(img, C):\n",
    "\t\"\"\" formats the image size based on config \"\"\"\n",
    "\timg_min_side = float(C.im_size)\n",
    "\t(height,width,_) = img.shape\n",
    "\t\t\n",
    "\tif width <= height:\n",
    "\t\tratio = img_min_side/width\n",
    "\t\tnew_height = int(ratio * height)\n",
    "\t\tnew_width = int(img_min_side)\n",
    "\telse:\n",
    "\t\tratio = img_min_side/height\n",
    "\t\tnew_width = int(ratio * width)\n",
    "\t\tnew_height = int(img_min_side)\n",
    "\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\treturn img, ratio\t\n",
    "\n",
    "def format_img_channels(img, C):\n",
    "\t\"\"\" formats the image channels based on config \"\"\"\n",
    "\timg = img[:, :, (2, 1, 0)]\n",
    "\timg = img.astype(np.float32)\n",
    "\timg[:, :, 0] -= C.img_channel_mean[0]\n",
    "\timg[:, :, 1] -= C.img_channel_mean[1]\n",
    "\timg[:, :, 2] -= C.img_channel_mean[2]\n",
    "\timg /= C.img_scaling_factor\n",
    "\timg = np.transpose(img, (2, 0, 1))\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\treturn img\n",
    "\n",
    "def format_img(img, C):\n",
    "\t\"\"\" formats an image for model prediction based on config \"\"\"\n",
    "\timg, ratio = format_img_size(img, C)\n",
    "\timg = format_img_channels(img, C)\n",
    "\treturn img, ratio\n",
    "\n",
    "\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "\treal_x1 = int(round(x1 // ratio))\n",
    "\treal_y1 = int(round(y1 // ratio))\n",
    "\treal_x2 = int(round(x2 // ratio))\n",
    "\treal_y2 = int(round(y2 // ratio))\n",
    "\n",
    "\treturn (real_x1, real_y1, real_x2 ,real_y2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('192.168.1.2', 1048)\n",
      "('127.0.0.1', 43150)\n"
     ]
    }
   ],
   "source": [
    "sock_server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "sock_server_cam = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "sock_server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)  # 重用地址端口\n",
    "sock_server_cam.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 2)\n",
    "sock_server.bind(('', 48952))\n",
    "sock_server_cam.bind(('127.0.0.1', 1234))   #和c++通信用\n",
    "\n",
    "sock_server.listen(1)  # 开始监听，1代表在允许有一个连接排队，更多的新连接连进来时就会被拒绝\n",
    "sock_server_cam.listen(1)\n",
    "\n",
    "conn, client_addr = sock_server.accept()  # 阻塞直到有连接为止，有了一个新连接进来后，就会为这个请求生成一个连接对象\n",
    "\n",
    "print(client_addr)\n",
    "\n",
    "conn_cam, client_addr_cam = sock_server_cam.accept()  # 阻塞直到有连接为止，有了一个新连接进来后，就会为这个请求生成一个连接对象\n",
    "\n",
    "print(client_addr_cam)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/hou/anaconda3/envs/bowang_srtp/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 512\n",
    "\n",
    "input_shape_img = (None, None, 3)\n",
    "input_shape_features = (None, None, num_features)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(C.num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "# define the base network (VGG here, can be Resnet50, Inception, etc)\n",
    "shared_layers = nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn_layers = rpn_layer(shared_layers, num_anchors)\n",
    "\n",
    "classifier = classifier_layer(feature_map_input, roi_input, C.num_rois, nb_classes=len(C.class_mapping))\n",
    "\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier_only = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "# print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "model_classifier.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch key value for class mapping\n",
    "class_mapping = C.class_mapping\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "# print(class_mapping)\n",
    "class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = os.listdir(test_base_path)\n",
    "\n",
    "imgs_path = []\n",
    "for i in range(12):\n",
    "\tidx = np.random.randint(len(test_imgs))\n",
    "\timgs_path.append(test_imgs[idx])\n",
    "\n",
    "all_imgs = []\n",
    "\n",
    "classes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cam.send('1'.encode('utf-8'))  # 相机拍照\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出结果（二维数组）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the box classification value is less than this, we ignore this box\n",
    "bbox_threshold = 0.7\n",
    "\n",
    "\n",
    "\n",
    "for idx, img_name in enumerate(imgs_path):\n",
    "    #if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n",
    "    #    continue\n",
    "    if not img_name==\"rs-save-to-disk-output-Color.png\":\n",
    "        continue\n",
    "    # print(img_name)\n",
    "    all_dets = []\n",
    "    \n",
    "    point_list = []   # 二维坐标系，分别放x（相对），y（相对），种类\n",
    "    st = time.time()\n",
    "    filepath = os.path.join(test_base_path, img_name)\n",
    "\n",
    "    img = cv2.imread(filepath)\n",
    "    \n",
    "    (imgHeight,imgWidth,_) = img.shape\n",
    "    X, ratio = format_img(img, C)\n",
    "    \n",
    "    X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "    # get output layer Y1, Y2 from the RPN and the feature maps F\n",
    "    # Y1: y_rpn_cls\n",
    "    # Y2: y_rpn_regr\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "    # Get bboxes by applying NMS \n",
    "    # R.shape = (300, 4)\n",
    "    R = rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), overlap_thresh=0.7)\n",
    "\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "        ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0]//C.num_rois:\n",
    "            #pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "        # Calculate bboxes coordinates on resized image\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "            # Ignore 'bg' class\n",
    "            if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "                tx /= C.classifier_regr_std[0]\n",
    "                ty /= C.classifier_regr_std[1]\n",
    "                tw /= C.classifier_regr_std[2]\n",
    "                th /= C.classifier_regr_std[3]\n",
    "                x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "\n",
    "\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "            # Calculate real coordinates on original image\n",
    "            (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "            XCenter = str(round((real_x1 + real_x2)/2/imgWidth,4))# 保留两位str小数，通讯需要str\n",
    "            YCenter = str(round((real_y1 + real_y2)/2/imgHeight,4))# 保留两位str小数，通讯需要str\n",
    "            point_list.append([XCenter,YCenter,str(key)])\n",
    "            \n",
    "            #newprobs = int(100 * new_probs[jk])\n",
    "            #if newprobs > oldprobs:\n",
    "            #    oldprobs = newprobs\n",
    "            #    point_list = [XCenter, YCenter, str(key)]\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)\n",
    "\n",
    "            textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "            all_dets.append((key,100*new_probs[jk]))\n",
    "\n",
    "            (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "            textOrg = (real_x1, real_y1-0)\n",
    "\n",
    "            cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)\n",
    "            cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "            cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "    print('Elapsed time = {}'.format(time.time() - st))\n",
    "    print(all_dets)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.grid()\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0.4793', '0.3729', 'COL'], ['0.5996', '0.3729', 'CUB'], ['0.5395', '0.0528', 'CUB'], ['0.4195', '0.2125', 'CUB']]\n"
     ]
    }
   ],
   "source": [
    "print(point_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3729\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(point_list[0][1])\n",
    "print(len(point_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(point_list)):\n",
    "    #conn_cam.send(point_list[i][0].encode('utf-8'))  # 发送x，y给相机\n",
    "    #conn_cam.send(point_list[i][1].encode('utf-8'))\n",
    "    \n",
    "    #depth = float(conn_cam.recv(1024)) / 10000     # 接收检测到的深度\n",
    "    #depth = str(depth * (-1000) + 560)\n",
    "    #if len(depth) > 8:\n",
    "    #    depth = depth[0:8]\n",
    "    #elif len(depth) < 8:\n",
    "    #    for x in range(8 - len(depth)):\n",
    "    #        depth += '0'\n",
    "    if float(point_list[i][0])>0.35 and float(point_list[i][0])<0.7 and float(point_list[i][1])>0.2 and float(point_list[i][1])<0.6:\n",
    "        point_list[i][1]=float(point_list[i][1]) * 480.0 + 205\n",
    "        point_list[i][0]=float(point_list[i][0]) * 990.0 - 533.0\n",
    "        x_temp = str(float(point_list[i][1]))\n",
    "        y_temp = str(float(point_list[i][0]))\n",
    "        if len(x_temp) > 8:\n",
    "            x_temp = x_temp[0:8]\n",
    "        elif len(x_temp) < 8:\n",
    "            for x in range(8 - len(x_temp)):\n",
    "                x_temp += '0'\n",
    "        if len(y_temp) > 8:\n",
    "            y_temp = y_temp[0:8]\n",
    "        elif len(y_temp) < 8:\n",
    "            for y in range(8 - len(y_temp)):\n",
    "                y_temp += '0'\n",
    "            \n",
    "    \n",
    "        conn.send(x_temp.encode('utf-8'))\n",
    "        conn.send(y_temp.encode('utf-8'))\n",
    "        conn.send(\"-100.000\".encode('utf-8'))\n",
    "        time.sleep(4)\n",
    "        grriper.setPosition(10)\n",
    "        time.sleep(1)# 抓住\n",
    "        if point_list[i][2]=='CUB':\n",
    "            conn.send('2'.encode('utf-8'))\n",
    "        elif point_list[i][2]=='COL':\n",
    "            conn.send('3'.encode('utf-8'))\n",
    "        else:\n",
    "            conn.send('4'.encode('utf-8'))\n",
    "\n",
    "\n",
    "        time.sleep(4)\n",
    "        grriper.setPosition(70)# 放下\n",
    "        conn.send('1'.encode('utf-8'))# 归位\n",
    "        time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
